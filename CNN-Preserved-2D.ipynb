{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style('white')\n",
    "plt.set_cmap('gist_gray')\n",
    "\n",
    "\n",
    "# We also install a package to read NiFTI files\n",
    "import nibabel as nib\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data as utils\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([536, 256, 10, 150])\n"
     ]
    }
   ],
   "source": [
    "#Saves the tensor\n",
    "# torch.save(new_img_list,\"../../../data/data_preserved.pt\")\n",
    "x = torch.load(\"../../../data/data_preserved.pt\")\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([536, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.load(\"../../../data/ages.pt\")\n",
    "n = y.shape[0]\n",
    "y = y.reshape((n,1))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 322 (0.60) training data points, 107 (0.20) validation data points,\u0007nd 107 (0.20) test data points\n"
     ]
    }
   ],
   "source": [
    "train_indices = [i for i in range(n) if (i % 5 == 0 or i % 5 == 2 or i % 5 == 4)]\n",
    "val_indices = [i for i in range(n) if i % 5 == 1]\n",
    "test_indices = [i for i in range(n) if i not in train_indices and i not in val_indices]\n",
    "\n",
    "print(\"There are %d (%.2f) training data points, %d (%.2f) validation data points,\\and %d (%.2f) test data points\" %(len(train_indices),len(train_indices)/n,len(val_indices),\\\n",
    "                                                                                                                  len(val_indices)/n,len(test_indices),len(test_indices)/n))\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(dataset, batch_size=4,\n",
    "                                             shuffle=False,sampler=utils.sampler.SubsetRandomSampler(train_indices))\n",
    "dataloaders['val'] = torch.utils.data.DataLoader(dataset, batch_size=4,\n",
    "                                             shuffle=False,sampler=utils.sampler.SubsetRandomSampler(val_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_offset = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_offset = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device,dtype = torch.float)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_offset += torch.sum(torch.abs(outputs - labels.data))\n",
    "\n",
    "            epoch_loss = running_loss / n\n",
    "            epoch_offset = running_offset / n\n",
    "\n",
    "            print('{} Loss: {:.4f} Diff: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_offset))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_offset < best_offset:\n",
    "                best_offset = epoch_offset\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Diff: {:4f}'.format(best_offset))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Conv2d(in_channels=256, out_channels=channel_1, kernel_size=5, stride=1, \\\n",
    "#               padding=2, bias=True),\n",
    "#     nn.Conv2d(in_channels=channel_1, out_channels=channel_2, kernel_size=5, stride=1, \\\n",
    "#               padding=2, bias=True),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "#     nn.Conv2d(in_channels=channel_2, out_channels=channel_1, kernel_size=3, stride=1, \\\n",
    "#               padding=1,bias=True),\n",
    "#     nn.Conv2d(in_channels=channel_1, out_channels=channel_2, kernel_size=3, stride=1, \\\n",
    "#               padding=1,bias=True),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "#     nn.Conv2d(in_channels=channel_2, out_channels=channel_1, kernel_size=3, stride=1, \\\n",
    "#               padding=1,bias=True),\n",
    "#     nn.Conv2d(in_channels=channel_1, out_channels=channel_2, kernel_size=3, stride=1, \\\n",
    "#               padding=1,bias=True),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "#     Flatten(),\n",
    "#     nn.Linear(channel_2 * 1 * 18, 1),\n",
    "# )\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(256, 32, 5, 1, padding=2, bias=True).to(device),\n",
    "    nn.Conv2d(32, 32, 5, 1, padding=2, bias=True).to(device),\n",
    "    nn.BatchNorm2d(32).to(device),\n",
    "    nn.AvgPool2d(kernel_size = 2, stride=2).to(device),\n",
    "    nn.Conv2d(32, 64,5,1,padding=2,bias=True).to(device),\n",
    "    nn.Conv2d(64, 64,5,1,padding=2,bias=True).to(device),\n",
    "    nn.BatchNorm2d(64).to(device),\n",
    "    nn.AvgPool2d(kernel_size = 2, stride=2).to(device),\n",
    "    nn.Conv2d(64, 128,3,1,padding=1,bias=True).to(device),\n",
    "    nn.Conv2d(128, 128,3,1,padding=1,bias=True).to(device),\n",
    "    nn.BatchNorm2d(128).to(device),\n",
    "    nn.AvgPool2d(kernel_size = 2, stride=2).to(device),\n",
    "    nn.Conv2d(128, 256,3,1,padding=1,bias=True).to(device),\n",
    "    nn.Conv2d(256, 256,3,1,padding=1,bias=True).to(device),\n",
    "    nn.BatchNorm2d(256).to(device),\n",
    "    nn.AvgPool2d(kernel_size = 2, stride=2).to(device),\n",
    "    nn.Conv2d(256, 512,3,1,padding=1,bias=True).to(device),\n",
    "    nn.Conv2d(512, 512,3,1,padding=1,bias=True).to(device),\n",
    "    nn.BatchNorm2d(512).to(device),\n",
    "    nn.AvgPool2d(kernel_size = 2, stride=2).to(device),\n",
    "    Flatten().to(device),\n",
    "    nn.Linear(512*1*2*2, 1).to(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "# num_ftrs = model_ft.fc.in_features\n",
    "# model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "# model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 453.1953 Diff: 13.3249\n",
      "val Loss: 60.0317 Diff: 2.7450\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 181.0023 Diff: 8.6898\n",
      "val Loss: 214.8189 Diff: 5.6045\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 163.7336 Diff: 8.4472\n",
      "val Loss: 88.3707 Diff: 3.4692\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 157.5674 Diff: 8.0678\n",
      "val Loss: 38.1821 Diff: 2.2918\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 145.8187 Diff: 7.7803\n",
      "val Loss: 42.5551 Diff: 2.4745\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 154.9249 Diff: 8.0269\n",
      "val Loss: 41.6361 Diff: 2.3441\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 138.7425 Diff: 7.7282\n",
      "val Loss: 39.2385 Diff: 2.2355\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 137.3955 Diff: 7.6796\n",
      "val Loss: 37.2601 Diff: 2.1968\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 131.7490 Diff: 7.4236\n",
      "val Loss: 39.7394 Diff: 2.2061\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 131.1912 Diff: 7.3346\n",
      "val Loss: 38.2005 Diff: 2.2162\n",
      "\n",
      "Training complete in 6m 48s\n",
      "Best val Diff: 2.196849\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_error(data,labels,model):\n",
    "    print('Checking error on test set')   \n",
    "    total_abs_err = 0.0\n",
    "    num_samples = data.shape[0]\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        labels = labels.to(device=device,dtype=dtype)\n",
    "        scores = model(data)\n",
    "        print((labels - scores).shape)\n",
    "        total_abs_err += torch.abs(labels - scores).sum()\n",
    "        mean_abs_err = float(total_abs_err) / num_samples\n",
    "        print('Got error %.4f' % (mean_abs_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([536, 1])\n",
      "Checking error on test set\n",
      "torch.Size([107, 1])\n",
      "Got error 11.0048\n"
     ]
    }
   ],
   "source": [
    "# y = torch.reshape(y,(n,1))\n",
    "check_error(x[val_indices,:], y[val_indices],model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
