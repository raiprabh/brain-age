{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style('white')\n",
    "plt.set_cmap('gist_gray')\n",
    "\n",
    "\n",
    "# We also install a package to read NiFTI files\n",
    "import nibabel as nib\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data as utils\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([536, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "#Saves the tensor\n",
    "# torch.save(new_img_list,\"../../../data/data_preserved.pt\")\n",
    "# x = torch.load(\"../../../data/data_preserved.pt\")\n",
    "\n",
    "# print(x.shape)\n",
    "x = torch.load(\"../../../data/data_64_3.pt\")\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([536, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.load(\"../../../data/ages.pt\")\n",
    "n = y.shape[0]\n",
    "y = y.reshape((n,1))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 322 (0.60) training data points, 107 (0.20) validation data points,\u0007nd 107 (0.20) test data points\n"
     ]
    }
   ],
   "source": [
    "train_indices = [i for i in range(n) if (i % 5 == 0 or i % 5 == 2 or i % 5 == 4)]\n",
    "val_indices = [i for i in range(n) if i % 5 == 3]\n",
    "test_indices = [i for i in range(n) if i not in train_indices and i not in val_indices]\n",
    "\n",
    "print(\"There are %d (%.2f) training data points, %d (%.2f) validation data points,\\and %d (%.2f) test data points\" %(len(train_indices),len(train_indices)/n,len(val_indices),\\\n",
    "                                                                                                                len(val_indices)/n,len(test_indices),len(test_indices)/n))\n",
    "train_dataset = utils.TensorDataset(x[train_indices],y[train_indices])\n",
    "val_dataset = utils.TensorDataset(x[val_indices],y[val_indices])      \n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(train_dataset, batch_size=4,\n",
    "                                             shuffle=True,num_workers=4)\n",
    "dataloaders['val'] = torch.utils.data.DataLoader(val_dataset, batch_size=4,\n",
    "                                             shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_offset = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_offset = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device,dtype = torch.float)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_offset += torch.sum(torch.abs(outputs - labels.data))\n",
    "\n",
    "            if phase == 'train':\n",
    "                epoch_loss = running_loss / (n * 0.6)\n",
    "                epoch_offset = running_offset / (n * 0.6)\n",
    "               \n",
    "            else:\n",
    "                epoch_loss = running_loss / (n * 0.2)\n",
    "                epoch_offset = running_offset / (n * 0.2)\n",
    "            \n",
    "\n",
    "            print('{} Loss: {:.4f} Diff: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_offset))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_offset < best_offset:\n",
    "                best_offset = epoch_offset\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Diff: {:4f}'.format(best_offset))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Conv2d(in_channels=256, out_channels=channel_1, kernel_size=5, stride=1, \\\n",
    "#               padding=2, bias=True),\n",
    "#     nn.Conv2d(in_channels=channel_1, out_channels=channel_2, kernel_size=5, stride=1, \\\n",
    "#               padding=2, bias=True),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "#     nn.Conv2d(in_channels=channel_2, out_channels=channel_1, kernel_size=3, stride=1, \\\n",
    "#               padding=1,bias=True),\n",
    "#     nn.Conv2d(in_channels=channel_1, out_channels=channel_2, kernel_size=3, stride=1, \\\n",
    "#               padding=1,bias=True),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "#     nn.Conv2d(in_channels=channel_2, out_channels=channel_1, kernel_size=3, stride=1, \\\n",
    "#               padding=1,bias=True),\n",
    "#     nn.Conv2d(in_channels=channel_1, out_channels=channel_2, kernel_size=3, stride=1, \\\n",
    "#               padding=1,bias=True),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "#     Flatten(),\n",
    "#     nn.Linear(channel_2 * 1 * 18, 1),\n",
    "# )\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(64, 32, 5, 1, padding=2, bias=True).to(device),\n",
    "    nn.Conv2d(32, 32, 5, 1, padding=2, bias=True).to(device),\n",
    "    nn.BatchNorm2d(32).to(device),\n",
    "    nn.AvgPool2d(kernel_size = 2, stride=2).to(device),\n",
    "    nn.Conv2d(32, 64,5,1,padding=2,bias=True).to(device),\n",
    "    nn.Conv2d(64, 64,5,1,padding=2,bias=True).to(device),\n",
    "    nn.BatchNorm2d(64).to(device),\n",
    "    nn.AvgPool2d(kernel_size = 2, stride=2).to(device),\n",
    "    nn.Conv2d(64, 128,3,1,padding=1,bias=True).to(device),\n",
    "    nn.Conv2d(128, 128,3,1,padding=1,bias=True).to(device),\n",
    "    nn.BatchNorm2d(128).to(device),\n",
    "    nn.AvgPool2d(kernel_size = 2, stride=2).to(device),\n",
    "    nn.Conv2d(128, 256,3,1,padding=1,bias=True).to(device),\n",
    "    nn.Conv2d(256, 256,3,1,padding=1,bias=True).to(device),\n",
    "    nn.BatchNorm2d(256).to(device),\n",
    "    nn.AvgPool2d(kernel_size = 2, stride=2).to(device),\n",
    "    nn.Conv2d(256, 512,3,1,padding=1,bias=True).to(device),\n",
    "    nn.Conv2d(512, 512,3,1,padding=1,bias=True).to(device),\n",
    "    nn.BatchNorm2d(512).to(device),\n",
    "    nn.AvgPool2d(kernel_size = 2, stride=2).to(device),\n",
    "    Flatten().to(device),\n",
    "    nn.Linear(512*1*2*2, 1).to(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "# num_ftrs = model_ft.fc.in_features\n",
    "# model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "# model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 760.6533 Diff: 21.6383\n",
      "val Loss: 2848.0234 Diff: 49.6464\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 230.8124 Diff: 12.1872\n",
      "val Loss: 213.9583 Diff: 12.4234\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 197.2146 Diff: 11.4923\n",
      "val Loss: 330.0891 Diff: 14.8748\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 212.5860 Diff: 11.9435\n",
      "val Loss: 249.7138 Diff: 12.9055\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 197.5732 Diff: 11.5805\n",
      "val Loss: 208.5511 Diff: 12.0164\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 197.9519 Diff: 11.2985\n",
      "val Loss: 205.6681 Diff: 12.2909\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 171.7247 Diff: 10.8559\n",
      "val Loss: 190.3245 Diff: 11.4894\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 166.1309 Diff: 10.6432\n",
      "val Loss: 189.9448 Diff: 11.5321\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 175.3148 Diff: 10.7761\n",
      "val Loss: 194.4403 Diff: 11.6914\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 168.4462 Diff: 10.5314\n",
      "val Loss: 194.1175 Diff: 11.6177\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 159.2187 Diff: 10.1914\n",
      "val Loss: 198.4221 Diff: 11.7053\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 166.1475 Diff: 10.3702\n",
      "val Loss: 195.3239 Diff: 11.6230\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 160.4016 Diff: 10.2652\n",
      "val Loss: 190.8524 Diff: 11.6203\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 161.0686 Diff: 10.2064\n",
      "val Loss: 192.2179 Diff: 11.6447\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 161.1193 Diff: 10.1520\n",
      "val Loss: 192.6523 Diff: 11.6329\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 171.8572 Diff: 10.5765\n",
      "val Loss: 194.8669 Diff: 11.6772\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 158.1516 Diff: 10.2023\n",
      "val Loss: 195.7261 Diff: 11.7112\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 162.4080 Diff: 10.4360\n",
      "val Loss: 194.1911 Diff: 11.6616\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 156.4439 Diff: 10.0900\n",
      "val Loss: 196.9438 Diff: 11.7520\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 161.2742 Diff: 10.1571\n",
      "val Loss: 194.6790 Diff: 11.6518\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 156.4582 Diff: 10.2059\n",
      "val Loss: 194.7178 Diff: 11.6536\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 162.1248 Diff: 10.3407\n",
      "val Loss: 194.4661 Diff: 11.6814\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 150.5207 Diff: 9.9642\n",
      "val Loss: 194.5144 Diff: 11.6512\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 163.5267 Diff: 10.3071\n",
      "val Loss: 193.5664 Diff: 11.6400\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 168.4359 Diff: 10.4289\n",
      "val Loss: 194.3922 Diff: 11.6514\n",
      "\n",
      "Training complete in 4m 38s\n",
      "Best val Diff: 11.489352\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_error(data,labels,model):\n",
    "    print('Checking error on test set')   \n",
    "    total_abs_err = 0.0\n",
    "    num_samples = data.shape[0]\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        labels = labels.to(device=device,dtype=dtype)\n",
    "        scores = model(data)\n",
    "        print((labels - scores).shape)\n",
    "        total_abs_err += torch.abs(labels - scores).sum()\n",
    "        mean_abs_err = float(total_abs_err) / num_samples\n",
    "        print('Got error %.4f' % (mean_abs_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking error on test set\n",
      "torch.Size([107, 1])\n",
      "Got error 12.1053\n"
     ]
    }
   ],
   "source": [
    "# y = torch.reshape(y,(n,1))\n",
    "check_error(x[val_indices,:], y[val_indices],model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"best_2d_model_preserved.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
