{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n",
      "536\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we plot one slice of the MRI of a subject\n",
    "\n",
    "# We first load the 2 files associated to raw data and segmentation\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#Collect all pre-processed images from .anat directories\n",
    "skullstripped_img_list = []\n",
    "ori_img_list = []\n",
    "seg_img_list = []\n",
    "file_id = []\n",
    "sample_size = 0\n",
    "root_path = \"/shared/ixi-dataset/\"\n",
    "for i in range(19):\n",
    "    dir_name = root_path + \"IXI-T1-\" + str(i)\n",
    "    anat_directories = [x for x in os.listdir(dir_name) if x.endswith('.anat')]\n",
    "    for anat_directory in anat_directories:\n",
    "        ori_file_name = dir_name + '/' + anat_directory + '/T1_biascorr.nii.gz'\n",
    "        seg_file_name = dir_name + '/' + anat_directory + \"/T1_fast_seg.nii.gz\"\n",
    "        if anat_directory[0:3] == 'IXI' and os.path.exists(ori_file_name) and os.path.exists(seg_file_name):\n",
    "            ori_img = nib.load(ori_file_name).get_data()\n",
    "            seg_img = nib.load(seg_file_name).get_data()\n",
    "            skullstripped_img = ori_img * (seg_img > 0)\n",
    "            ori_img_list.append(ori_img)\n",
    "            seg_img_list.append(seg_img)\n",
    "            skullstripped_img = (skullstripped_img - np.mean(skullstripped_img)) / np.std(skullstripped_img)\n",
    "            skullstripped_img_list.append(skullstripped_img)\n",
    "            file_id.append(anat_directory[3:6])\n",
    "            sample_size += 1\n",
    "            clear_output(wait=True)\n",
    "            #print(sample_size)\n",
    "print(len(file_id))\n",
    "print(len(skullstripped_img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, size=224):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
    "                    std=SQUEEZENET_STD.tolist()),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def deprocess(img, should_rescale=True):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda x: x[0]),\n",
    "        T.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),\n",
    "        T.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),\n",
    "        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n",
    "        T.ToPILImage(),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def rescale(x):\n",
    "    low, high = x.min(), x.max()\n",
    "    x_rescaled = (x - low) / (high - low)\n",
    "    return x_rescaled\n",
    "    \n",
    "def blur_image(X, sigma=1):\n",
    "    X_np = X.cpu().clone().numpy()\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n",
    "    X.copy_(torch.Tensor(X_np).type_as(X))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the pretrained SqueezeNet model.\n",
    "# model = torchvision.models.resnet50(pretrained=True)\n",
    "model = torch.load('./the_whole_model.pth')\n",
    "# We don't want to train the model, so tell PyTorch not to compute gradients\n",
    "# with respect to model parameters.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# you may see warning regarding initialization deprecated, that's fine, please continue to next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1697,  0.5575,  0.6798,  0.4442, -0.0081],\n",
      "        [ 0.4141, -0.3252,  0.7232, -0.2690, -1.5573],\n",
      "        [ 0.3095, -0.7394,  0.0028, -0.4891,  0.6778],\n",
      "        [ 1.1790, -1.2183, -0.4638,  0.9921,  0.8412]])\n",
      "tensor([1, 2, 1, 3])\n",
      "tensor([ 0.5575,  0.7232, -0.7394,  0.9921])\n"
     ]
    }
   ],
   "source": [
    "# Example of using gather to select one entry from each row in PyTorch\n",
    "def gather_example():\n",
    "    N, C = 4, 5\n",
    "    s = torch.randn(N, C)\n",
    "    y = torch.LongTensor([1, 2, 1, 3])\n",
    "    print(s)\n",
    "    print(y)\n",
    "    print(s.gather(1, y.view(-1, 1)).squeeze())\n",
    "gather_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make input tensor require gradient\n",
    "    X.requires_grad_()\n",
    "    \n",
    "    saliency = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement this function. Perform a forward and backward pass through #\n",
    "    # the model to compute the gradient of the correct class score with respect  #\n",
    "    # to each input image. You first want to compute the loss over the correct   #\n",
    "    # scores (we'll combine losses across a batch by summing), and then compute  #\n",
    "    # the gradients with a backward pass.                                        #\n",
    "    ##############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    scores = torch.LongTensor(model.forward(X))\n",
    "    correct_class_scores = scores.gather(1, y.view(-1, 1)).squeeze()\n",
    "    loss = torch.sum(correct_class_scores)\n",
    "    loss.backward()\n",
    "    grads = X.grad\n",
    "    saliency,_ = torch.max(grads.abs(),1)\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_saliency_maps(X, y):\n",
    "    # Convert X and y from numpy arrays to Torch Tensors\n",
    "    X_tensor = torch.from_numpy(X)\n",
    "    X_tensor = X_tensor.to(device,dtype=dtype)\n",
    "    y = y.to(device,dtype=dtype)\n",
    "#     y_tensor = torch.from_numpy(y)\n",
    "\n",
    "    # Compute saliency maps for images in X\n",
    "    saliency = compute_saliency_maps(X_tensor, y, model)\n",
    "    \n",
    "    s = int(X.shape[2] / 2)\n",
    "    dimension = 2\n",
    "    # Convert the saliency map from Torch Tensor to numpy array and show images\n",
    "    # and saliency maps together.\n",
    "    saliency = saliency.numpy()\n",
    "    N = X.shape[0]\n",
    "    for i in range(N):\n",
    "        plt.subplot(2, N, i + 1)\n",
    "        plt.imshow(X[i])\n",
    "        plt.imshow(np.take(X[i], s, dimension))\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y[i]])\n",
    "        plt.subplot(2, N, N + i + 1)\n",
    "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(12, 5)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the MRI : (536, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "new_img_list = []\n",
    "\n",
    "for i in range(len(skullstripped_img_list)):    \n",
    "    new_img = skullstripped_img_list[i][16:240,90,:]\n",
    "    #modify the dimensions by cropping and zero-padding so that all the images' sizes become 224*224\n",
    "    d_2 = new_img.shape[1]\n",
    "    if d_2 % 2 == 0:\n",
    "        pad_width = int((224 - d_2) / 2)\n",
    "        new_img = np.pad(np.asarray(new_img),((0,0),(pad_width, pad_width)),\"constant\") \n",
    "    else: \n",
    "        pad_width_0 = int((224 - d_2) / 2)\n",
    "        pad_width_1 = pad_width_0 + 1\n",
    "        new_img = np.pad(np.asarray(new_img),((0,0),(pad_width_0, pad_width_1)),\"constant\") \n",
    "    final_img = np.tile(new_img,(3,1)).reshape((3,224,224))\n",
    "    new_img_list.append(final_img)    \n",
    "new_img_list = np.asarray(new_img_list)\n",
    "print('Shape of the MRI : {}'.format(new_img_list.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(536, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "print(new_img_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.17 GiB total capacity; 156.02 MiB already allocated; 5.06 MiB free; 1.98 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6aa42268b1a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../../data/data.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../../data/labels_1.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshow_saliency_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_img_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-3dd4a6d73d47>\u001b[0m in \u001b[0;36mshow_saliency_maps\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Convert X and y from numpy arrays to Torch Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     y_tensor = torch.from_numpy(y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.17 GiB total capacity; 156.02 MiB already allocated; 5.06 MiB free; 1.98 MiB cached)"
     ]
    }
   ],
   "source": [
    "X = torch.load(\"../../../data/data.pt\")\n",
    "y = torch.load(\"../../../data/labels_1.pt\")\n",
    "show_saliency_maps(new_img_list[0:2], y[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
